/*	Author: Dexter Barrows
	Github: dbarrows.github.io
	*/

/*	Runs a particle filter on synthetic noisy data and attempts to
	reconstruct underlying true state at each time step. Note that
	this program uses gnuplot to plot the data, so an x11
	environment must be present. Also the multiplier of 1024 in the
	definition of NP below should be set to a multiple of the number
	of multiprocessors of your GPU for optimal results.

	Also, the accompanying "pf.plg" file contains the instructions
	gnuplot will use. It must be present in the same directory as
	the executable generated by compiling this file.

	Compile with:

	nvcc -arch=sm_20 -O2 pf_cuda.cu timer.cpp rand.cpp -o pf_cuda.x

	*/

#include <cuda.h>
#include <iostream>
#include <fstream>
#include <curand.h>
#include <curand_kernel.h>
#include <string>
#include <sstream>

#include "timer.h"
#include "rand.h"
#include "readdata.h"

#define NP 			(2*2500) 	// number of particles
#define N 			500.0		// population size
#define Treal 		100			// time to simulate over
#define R0true 		3.0			// infectiousness
#define rtrue 		0.1			// recovery rate
#define Nreal 		500.0 		// population size
#define etatrue 	0.5			// real drift attraction strength
#define berrtrue	0.5			// real beta drift noise
#define phitrue 	0.5 		// real connectivity strength
#define merr 		10.0  		// expected measurement error
#define I0 			5.0			// Initial infected individuals
#define PSC 		0.5 		// sensitive parameter perturbation scaling

#define RB_DIM 	1 				// size of the reduction blocks

#define PI 		3.141592654f

// Wrapper for CUDA calls, from CUDA API
// Modified to also print the error code and string
# define CUDA_CALL(x) do { if ((x) != cudaSuccess ) {					\
	std::cout << " Error at " << __FILE__ << ":" << __LINE__ << std::endl;		\
	std::cout << " Error was " << x << " " << cudaGetErrorString(x) << std::endl;	\
	return EXIT_FAILURE ;}} while (0)									\

typedef struct {
	float R0;
	float r;
	float sigma;
	float eta;
	float berr;
	float phi;
	float * S;
	float * I;
	float * R;
	float * B;
	float * Iinit;
	curandState randState; 	// PRNG state
} Particle;

__host__ std::string getHRmemsize (size_t memsize);
__host__ std::string getHRtime (float runtime);

__device__ void exp_euler_SSIR(float h, float t0, float tn, Particle * particle, int * neinum, int * neibmat, int nloc);
__device__ void copyParticle(Particle * dst, Particle * src, int nloc);


/* 	Initialize all PRNG states, get starting state vector using initial distribution
	*/
__global__ void initializeParticles (Particle * particles, int nloc) {

	int id 	= blockIdx.x*blockDim.x + threadIdx.x;	// global thread ID

	// initialize PRNG state
	curandState state;
	curand_init(id, 0, 0, &state);
	
	// allocate space for arays inside particle
	particles[id].S = (float*) malloc(nloc*sizeof(float));
	particles[id].I = (float*) malloc(nloc*sizeof(float));
	particles[id].R = (float*) malloc(nloc*sizeof(float));
	particles[id].B = (float*) malloc(nloc*sizeof(float));
	particles[id].Iinit = (float*) malloc(nloc*sizeof(float));

	// initialize all parameters

	float R0can, rcan, sigmacan, Iinitcan, etacan, berrcan, phican;

	do {
		R0can = R0true + R0true*curand_normal(&state);
	} while (R0can < 0);
	particles[id].R0 = R0can;

	do {
		rcan = rtrue + rtrue*curand_normal(&state);
	} while (rcan < 0);
	particles[id].r = rcan;

	for (int loc = 0; loc < nloc; loc++)
		particles[id].B[loc] = (float) R0can * rcan / N;

	do {
		sigmacan = merr + merr*curand_normal(&state);
	} while (sigmacan < 0);
	particles[id].sigma = sigmacan;

	do {
		etacan = etatrue + PSC*etatrue*curand_normal(&state);
	} while (etacan < 0 || etacan > 1);
	particles[id].eta = etacan;

	do {
		berrcan = berrtrue + PSC*berrtrue*curand_normal(&state);
	} while (berrcan < 0);
	particles[id].berr = berrcan;

	do {
		phican = phitrue + PSC*phitrue*curand_normal(&state);
	} while (phican <= 0 || phican >= 1);
	particles[id].phi = phican;

	for (int loc = 0; loc < nloc; loc++) {
		do {
			Iinitcan = I0 + I0*curand_normal(&state);
		} while (Iinitcan < 0 || N < Iinitcan);
		particles[id].Iinit[loc] = Iinitcan;
	}

}


/* 	Project particles forward, perturb, and save weight based on data
	int t - time step number (1,...,T)
	*/
__global__ void project (int * data, int t, int T, Particle * particles, Particle * particles_old, float * w,
						 int * neinum, int * neibmat, int nloc) {

	int id = blockIdx.x*blockDim.x + threadIdx.x;	// global id

	// project forward
	//exp_euler_SIR(1.0/10, 0.0, 1.0, particles, id, &E[t*nCells], Beta_last, adjMat, nNeibVec, cID);
	exp_euler_SSIR(1.0/7.0, 0.0, 1.0, &particles[id], neinum, neibmat, nloc);
	//exp_euler_SSIR(float h, float t0, float tn, int N, Particle * particle, int * neinum, int * neibmat, int nloc);


	float merr_par = particles[id].sigma;

	// Get weight and save
	w[id] = 1.0;
	for (int loc = 0; loc < nloc; loc++) {
		float y_diff = data[nloc*T + t] - particles[id].I[loc];
		w[id] *= 1.0/(merr_par*sqrt(2.0*PI)) * exp( - y_diff*y_diff / (2.0*merr_par*merr_par) );
	}

	// COPY PARTICLE
	copyParticle(&particles_old[id], &particles_old[id], nloc);

}


/* 	The 0th thread will perform cumulative sum on the weights.
	There may be a faster way to do this, will investigate.
	*/
__global__ void cumsumWeights (float * w, int nCells) {

	int id 	= blockIdx.x*blockDim.x + threadIdx.x;	// global thread ID

	// compute cumulative weights
	if (id == 0) {
		for (int i = 1; i < NP; i++)
			w[i] += w[i-1];
	}

}


/* 	Resample from all particle states within cell
	*/
__global__ void resample (Particle * particles, Particle * particles_old, float * w, int nloc) {

	int id 	= blockIdx.x*blockDim.x + threadIdx.x;

	// resampling proportional to weights
	float w_r = curand_uniform(&particles[id].randState) * w[NP-1];
	int i = 0;
	while (w_r > w[i]) {
		i++;
	}	

	// i is now the index of the particle to copy from
	copyParticle(&particles[id], &particles_old[i], nloc);

}



int main (int argc, char *argv[]) {


	int T, nloc;

	double restime;
	struct timeval tdr0, tdr1, tdrMaster;


	// Parse arguments **********************************************

	if (argc < 4) {
		std::cout << "Not enough arguments" << std::endl;
		return 0;
	}

	std::string arg1(argv[1]); 	// infection counts
	std::string arg2(argv[2]);	// neighbour counts
	std::string arg3(argv[3]);	// neighbour indices

	std::cout << "Arguments:" << std::endl;
	std::cout << "Infection data: 	 [1]" << arg1 << std::endl;
	std::cout << "Neighbour counts:  [2]" << arg2 << std::endl;
	std::cout << "Neighbour indices: [3]" << arg3 << std::endl;

	// **************************************************************


	exit(EXIT_SUCCESS);


	// Read count data **********************************************

	std::cout << "Getting count data" << std::endl;
	float * data = getDataFloat(arg1, &T, &nloc);
	size_t datasize = nloc*T*sizeof(float);

	// **************************************************************

	// Read neinum matrix data **************************************

	std::cout << "Getting neighbour count data" << std::endl;
	int * neinum = getDataInt(arg2, NULL, NULL);
	size_t neinumsize = nloc * sizeof(int);

	// **************************************************************

	// Read neibmat matrix data *************************************

	std::cout << "Getting neighbour count data" << std::endl;
	int * neibmat = getDataInt(arg3, NULL, NULL);
	size_t neibmatsize = nloc * nloc * sizeof(int);

	// **************************************************************
	

	gettimeofday (&tdr1, NULL);
    timeval_subtract (&restime, &tdr1, &tdr0);

    std::cout << "\t" << getHRtime(restime) << std::endl;

	// *****************************************************************************************************


	// CUDA data ****************************************************

	std::cout << "Allocating device storage" << std::endl;

	gettimeofday (&tdr0, NULL);

	int 		* d_data;			// device copy of data
	Particle 	* particles;		// particles
	Particle 	* particles_old; 	// intermediate particle states
	float 		* w;				// weights
	int         * d_neinum; 		// device copy of adjacency matrix
	int 		* d_neibmat; 		// device copy of neighbour counts matrix

	CUDA_CALL( cudaMalloc( (void**) &d_data 		, datasize )			);
	CUDA_CALL( cudaMalloc( (void**) &particles 		, NP*sizeof(Particle)) 	);
	CUDA_CALL( cudaMalloc( (void**) &particles_old 	, NP*sizeof(Particle)) 	);
	CUDA_CALL( cudaMalloc( (void**) &w 				, NP*sizeof(float)) 	);
	CUDA_CALL( cudaMalloc( (void**) &d_neinum 		, neinumsize) 			);
	CUDA_CALL( cudaMalloc( (void**) &d_neibmat 		, neibmatsize) 			);

	gettimeofday (&tdr1, NULL);
    timeval_subtract (&restime, &tdr1, &tdr0);

    std::cout << "\t" << getHRtime(restime) << std::endl;

	size_t avail, total;
	cudaMemGetInfo( &avail, &total );
	size_t used = total - avail;

	std::cout << "\t[" << getHRmemsize(used) << "] used of [" << getHRmemsize(total) << "]" <<std::endl;

	std::cout << "Copying data to device" << std::endl;

	gettimeofday (&tdr0, NULL);

	CUDA_CALL( cudaMemcpy(d_data	, data 		, datasize		, cudaMemcpyHostToDevice)	);
	CUDA_CALL( cudaMemcpy(d_neinum	, neinum 	, neinumsize	, cudaMemcpyHostToDevice)	);
	CUDA_CALL( cudaMemcpy(d_neibmat , neibmat 	, neibmatsize	, cudaMemcpyHostToDevice)	);

	gettimeofday (&tdr1, NULL);
    timeval_subtract (&restime, &tdr1, &tdr0);

    std::cout << "\t" << getHRtime(restime) << std::endl;

	// **************************************************************


	// Initialize particles *****************************************

	std::cout << "Initializing particles" << std::endl;

	gettimeofday (&tdr0, NULL);

	int nThreads 	= 256;
	int nBlocks 	= (NP / nThreads) + (NP % nThreads);

	initializeParticles <<< nBlocks, nThreads >>> (particles, nloc);
	initializeParticles <<< nBlocks, nThreads >>> (particles_old, nloc);
	CUDA_CALL( cudaGetLastError() );
	CUDA_CALL( cudaDeviceSynchronize() );

	gettimeofday (&tdr1, NULL);
    timeval_subtract (&restime, &tdr1, &tdr0);

    std::cout << "\t" << getHRtime(restime) << std::endl;

	// **************************************************************


	// Initial estimate by reduction ********************************

    /*
	nThreads 	= RB_DIM;
	nBlocks 	= nCells;

	reduce <<< nBlocks, nThreads >>> (d_E, 0, particles, Beta_last, nCells);
	CUDA_CALL( cudaGetLastError() );
	CUDA_CALL( cudaDeviceSynchronize() );

	float * h_E;
	cudaMallocHost( (void**) &h_E, nCells*T*sizeof(float) );
	cudaMemcpy(h_E, d_E, nCells*T*sizeof(float), cudaMemcpyDeviceToHost);
	*/

		/*std::cout << "======" << std::endl;
		std::cout << "T = " << 0 << std::endl;
		std::cout << "------" << std::endl;
		for(int i = 0; i < nCells; i++) {
			std::cout << i << " | " << data[i] << "\t" << h_E[i] << std::endl;
		}*/

	// **************************************************************


	// Starting filtering *******************************************

	std::cout << "Filtering over [1," << T << "]"<< std::endl;


	gettimeofday (&tdrMaster, NULL);

	int T_lim = T;

	for (int t = 1; t < T_lim; t++) {

		// Projection ************************************************

		nThreads 	= 256;
		nBlocks 	= (NP / nThreads) + (NP % nThreads);

		if (t == 1)
			gettimeofday (&tdr0, NULL);

		//project <<< nBlocks, nThreads >>> (d_data, particles, particles_old, w, d_E, Beta_last, d_adjMat, d_nNeibVec, t-1, nCells);
		project <<< nBlocks, nThreads >>> (d_data, t, T, particles, particles_old, w, d_neinum, d_neibmat, nloc);
		CUDA_CALL( cudaGetLastError() );
		CUDA_CALL( cudaDeviceSynchronize() );

		if (t == 1) {
			gettimeofday (&tdr1, NULL);
	    	timeval_subtract (&restime, &tdr1, &tdr0);
	    	std::cout << "Projection       " << getHRtime(restime) << std::endl;
	    }

	    // Cumulative sum ********************************************

	    /*
		nThreads 	= 32;
		nBlocks 	= nCells / nThreads + ( nCells % nThreads );

		if (t == 1)
			gettimeofday (&tdr0, NULL);

		cumsumWeights <<< nBlocks, nThreads >>> (w, nCells);
		CUDA_CALL( cudaGetLastError() );
		CUDA_CALL( cudaDeviceSynchronize() );

		if (t == 1) {
			gettimeofday (&tdr1, NULL);
	    	timeval_subtract (&restime, &tdr1, &tdr0);
	    	std::cout << "Cumulative sum   " << getHRtime(restime) << std::endl;
	    } */

	    // Resampling *************************************************

		nThreads 	= 256;
		nBlocks 	= (NP/ nThreads) + (NP % nThreads);

		if (t == 1)
			gettimeofday (&tdr0, NULL);

		resample <<< nBlocks, nThreads >>> (particles, particles_old, w, nloc);
		CUDA_CALL( cudaGetLastError() );
		CUDA_CALL( cudaDeviceSynchronize() );

		if (t == 1) {
			gettimeofday (&tdr1, NULL);
	    	timeval_subtract (&restime, &tdr1, &tdr0);
	    	std::cout << "Resampling       " << getHRtime(restime) << std::endl;
	    }

	    // Reduction **************************************************

		/*
		nThreads 	= RB_DIM;
		nBlocks 	= nCells;

		if (t == 1)
			gettimeofday (&tdr0, NULL);

		reduce <<< nBlocks, nThreads >>> (d_E, t, particles, Beta_last, nCells);
		CUDA_CALL( cudaGetLastError() );
		CUDA_CALL( cudaDeviceSynchronize() );

		if (t == 1) {
			gettimeofday (&tdr1, NULL);
	    	timeval_subtract (&restime, &tdr1, &tdr0);
	    	std::cout << "Reduction        " << getHRtime(restime) << std::endl;
	    }
	    */

	    // Output *****************************************************

	    /*
		if (t == T_lim/2) {

			cudaMemcpy(h_E, d_E, nCells*T*sizeof(float), cudaMemcpyDeviceToHost);

			std::string filename = "cuSPF.dat";

			std::cout << "Writing results to file '" << filename << "' ..." << std::endl;

			std::ofstream outfile;
			outfile.open(filename.c_str());

			for (int i = 0; i < nCells; i++) {
				outfile << trueCounts[t*nCells + i];
				if (i % dim == 0)
					outfile << std::endl;
				else
					outfile << " ";
			}
			
			for (int i = 0; i < nCells; i++) {
				outfile << data[t*nCells + i];
				if (i % dim == 0)
					outfile << std::endl;
				else
					outfile << " ";
			}

			for (int i = 0; i < nCells; i++) {
				outfile << h_E[t*nCells + i];
				if (i % dim == 0)
					outfile << std::endl;
				else
					outfile << " ";
			}

			outfile.close();
			
		}
		*/

	}

	gettimeofday (&tdr1, NULL);
	timeval_subtract (&restime, &tdr1, &tdrMaster);
	std::cout << "Total PF time (excluding setup) " << getHRtime(restime) << std::endl;

	exit (EXIT_SUCCESS);

}


/*	Use the Explicit Euler integration scheme to integrate SIR model forward in time
	float h 	- time step size
	float t0 	- start time
	float tn 	- stop time
	float * y 	- current system state; a three-component vector representing [S I R], susceptible-infected-recovered
	*/
__device__ void exp_euler_SSIR(float h, float t0, float tn, Particle * particle, int * neinum, int * neibmat, int nloc) {

	int num_steps = floor( (tn-t0) / h );

	float * S = particle->S;
	float * I = particle->I;
	float * R = particle->R;
	float * B = particle->B;

	// create last state vectors
	//float S_last[nloc];
	float * S_last = (float*) malloc (nloc*sizeof(float));
	float * I_last = (float*) malloc (nloc*sizeof(float));
	float * R_last = (float*) malloc (nloc*sizeof(float));
	float * B_last = (float*) malloc (nloc*sizeof(float));

	float R0 	= particle->R0;
	float r 	= particle->r;
	float B0 	= R0 * r / N;
	float eta 	= particle->eta;
	float berr = particle->berr;
	float phi  = particle->phi;

	for(int t = 0; t < num_steps; t++) {

		for (int loc = 0; loc < nloc; loc++) {
			S_last[loc] = S[loc];
			I_last[loc] = I[loc];
			R_last[loc] = R[loc];
			B_last[loc] = B[loc];
		}

		for (int loc = 0; loc < nloc; loc++) {

			B[loc] = exp( log(B_last[loc]) + eta*(log(B0) - log(B_last[loc])) + berr*curand_normal(&(particle->randState)) );

			int n = neinum[loc];
        	float sphi = 1.0 - phi*( (float) n/(n+1.0) );
        	float ophi = phi/(n+1.0);

        	float nBIsum = 0.0;
        	for (int j = 0; j < n; j++)
        		nBIsum += B_last[neibmat[nloc*loc + j]] * I_last[neibmat[nloc*loc + j]];

        	float BSI = S_last[loc]*( sphi*B_last[loc]*I_last[loc] + ophi*nBIsum );
        	float rI  = r*I_last[loc];

			// get derivatives
			float dS = - BSI;
			float dI = BSI - rI;
			float dR = rI;

			// step forward by h
			S[loc] += h*dS;
			I[loc] += h*dI;
			R[loc] += h*dR;

		}

	}

}

/*	Convinience function for particle resampling process
	*/
__device__ void copyParticle(Particle * dst, Particle * src, int nloc) {

	dst->R0 	= src->R0;
	dst->r 		= src->r;
	dst->sigma 	= src->sigma;
	dst->eta 	= src->eta;
	dst->berr 	= src->berr;
	dst->phi 	= src->phi;

	for (int n = 0; n < nloc; n++) {
		dst->S[n]		= src->S[n];
		dst->I[n]		= src->I[n];
		dst->R[n] 		= src->R[n];
		dst->B[n]      	= src->B[n];
		dst->Iinit[n]  	= src->Iinit[n];
	}

}

/*	Convert memory size in bytes to human-readable format
	*/
std::string getHRmemsize (size_t memsize) {

	std::stringstream ss;
	std::string valstring;

	int kb = 1024;
	int mb = kb*1024;
	int gb = mb*1024;
	
	if (memsize <= kb)
		ss << memsize << " B";
	else if (memsize > kb && memsize <= mb)
		ss << (float) memsize/ kb << " KB";
	else if (memsize > mb && memsize <= gb)
		ss << (float) memsize/ mb << " MB";
	else
		ss << (float) memsize/ gb << " GB";

	valstring = ss.str();
	
	return valstring;

}


/*	Convert time in seconds to human readable format
	*/
std::string getHRtime (float runtime) {

	std::stringstream ss;
	std::string valstring;

	int mt = 60;
	int ht = mt*60;
	int dt = ht*24;
	
	if (runtime <= mt)
		ss << runtime << " s";
	else if (runtime > mt && runtime <= ht)
		ss << runtime/mt << " m";
	else if (runtime > ht && runtime <= dt)
		ss << runtime/dt << " h";
	else
		ss << runtime/ht << " d";

	valstring = ss.str();
	
	return valstring;

}