/*	Author: Dexter Barrows
	Github: dbarrows.github.io
	*/

/*	Runs a particle filter on synthetic noisy data and attempts to
	reconstruct underlying true state at each time step. Note that
	this program uses gnuplot to plot the data, so an x11
	environment must be present. Also the multiplier of 1024 in the
	definition of NP below should be set to a multiple of the number
	of multiprocessors of your GPU for optimal results.

	Also, the accompanying "pf.plg" file contains the instructions
	gnuplot will use. It must be present in the same directory as
	the executable generated by compiling this file.

	Compile with:

	nvcc -arch=sm_20 -O2 pf_cuda.cu timer.cpp rand.cpp -o pf_cuda.x

	*/

#include <cuda.h>
#include <iostream>
#include <fstream>
#include <curand.h>
#include <curand_kernel.h>
#include <string>
#include <sstream>

#include "timer.h"
#include "rand.h"
#include "readdata.h"

#define NP 		(5*1024) 	// number of particles, should optimally be a multiple of 1024*NMP,
							// where NMP is the number of multiprocessors on the cuda device
//#define T 		100			// time to simulate over
#define R0 		3.0			// infectiousness
#define r 		1e-1		// recovery rate
#define N 		500 		// population size
#define B 		R0*r/N		// transmission factor
#define merr 	5  			// measurement error
#define eta 	0.5			// Beta drift cooling
#define alp		1e-2		// neighbor effect strength

#define RB_DIM 	1 			// size of the reduction blocks

#define PI 		3.141592654f
#define RMAX    4294967296

// Wrapper for CUDA calls, from CUDA API
// Modified to also print the error code and string
# define CUDA_CALL(x) do { if ((x) != cudaSuccess ) {					\
	std::cout << " Error at " << __FILE__ << ":" << __LINE__ << std::endl;		\
	std::cout << " Error was " << x << " " << cudaGetErrorString(x) << std::endl;	\
	return EXIT_FAILURE ;}} while (0)									\

struct Particle {
	float S;				// Susceptible
	float I;				// Infected
	float R;				// Recovered
	float Beta;				// Force of infection
	curandState randState; 	// PRNG state
};

__device__
void exp_euler_SIR(float h, float t0, float tn,
					Particle * particles, int pID,
					float * I_last, float * Beta_last, int * adjMat, int * nNeibVec, int cID);

__host__ std::string getHRmemsize (size_t memsize);
__host__ std::string getHRtime (double runtime);


/* 	Initialize all PRNG states, get starting state vector using initial distribution
	*/
__global__ void initializeParticles (int * data, Particle * particles) {

	int id 	= blockIdx.x*blockDim.x + threadIdx.x;	// global thread ID
	int cID = id / NP;								// cell ID

	// initialize PRNG state
	curandState state;
	curand_init(id, 0, 0, &state);

	// initial piece of data is 0'th for that cell
	int datapoint = data[cID];

	float I_init = datapoint + merr*curand_normal(&state);
	if (I_init < 0)
		I_init = 0;
	float S_init = N - I_init;

	float Beta = 2*B*curand_uniform(&state);

	// save particle state
	particles[id].S 			= S_init;
	particles[id].I 			= I_init;
	particles[id].R 			= 0;
	particles[id].Beta			= Beta;
	particles[id].randState 	= state;

}


/* 	Project particles forward, perturb, and save weight based on data
	int t - time step number (1,...,T)
	*/
__global__ void project (int * data, Particle * particles, Particle * particles_old, float * w, float * E,
						float * Beta_last, int * adjMat, int * nNeibVec, int t, int nCells) {

	int id 	= blockIdx.x*blockDim.x + threadIdx.x;	// global id
	int cID = id / NP;								// cell ID
	//int pID = id % NP;							// particle within cell

	int datapoint = data[(t+1)*nCells + cID];

	// project forward
	exp_euler_SIR(1.0/10, 0.0, 1.0, particles, id, &E[t*nCells], Beta_last, adjMat, nNeibVec, cID);

	// perturb with expected measurement noise
	float y_noise = particles[id].I + (float) merr*curand_normal(&particles[id].randState);
	if (y_noise < 0)
		y_noise = 0;

	float y_diff = datapoint - y_noise;

	// Get weight and save
	// Could generalize this using a user-supplied liklihood function
	w[id] = 1.0/(merr*sqrt(2.0*PI)) * expf( - y_diff*y_diff / (2.0*merr*merr) );

	particles_old[id].S 	= particles[id].S;
	particles_old[id].I 	= particles[id].I;
	particles_old[id].R 	= particles[id].R;
	particles_old[id].Beta 	= particles[id].Beta;

}


/* 	The 0th thread will perform cumulative sum on the weights.
	There may be a faster way to do this, will investigate.
	*/
__global__ void cumsumWeights (float * w, int nCells) {

	int id 	= blockIdx.x*blockDim.x + threadIdx.x;	// global thread ID

	int offset = id*NP;

	if (id < nCells) {
		for (int i = 1; i < NP; i++)
			w[offset + i] += w[offset + i - 1];
	}

}


/* 	Resample from all particle states within cell
	*/
__global__ void resample (Particle * particles, Particle * particles_old, float * w) {

	int id 	= blockIdx.x*blockDim.x + threadIdx.x;	// global id
	int cID = id / NP;								// cell ID

	int offset = cID*NP;							// offset to beginning of this cell's weights

	// resampling proportional to weights
	float w_r = curand_uniform(&particles[id].randState) * w[offset + NP - 1];
	int i = 0;
	while (w_r > w[offset + i]) {
		i++;
	}	

	// i is now the particle to copy from
	particles[id].S 	= particles_old[offset + i].S;
	particles[id].I 	= particles_old[offset + i].I;
	particles[id].R 	= particles_old[offset + i].R;
	particles[id].Beta 	= particles_old[offset + i].Beta;

}


/* 	Reduce all the estimates made at each time step into a sequence of mean estimates
	*/
__global__ void reduce (float * E, int t, Particle * particles, float * Beta_last, int nCells) {

	__shared__ float sums[RB_DIM];
	__shared__ float Bsums[RB_DIM];

	int cID = blockIdx.x;
	int tID = threadIdx.x;
	
	int window 	= NP/RB_DIM;			// how many particles each thread should sum over
	int offset 	= cID*NP + tID*window;	// offset to beginning of thread's set of particles

	float sum = 0;
	float Bsum = 0;
	for (int i = 0; i < window; i++) {
		sum 	+= particles[ offset + i ].I;
		Bsum 	+= particles[ offset + i ].Beta;
	}

	sums[tID] = sum;
	Bsums[tID] = Bsum;

	__syncthreads();

	if (tID == 0) {
		float sum = 0;
		float Bsum = 0;
		for (int i = 0; i < RB_DIM; i++) {
			sum += sums[i];
			Bsum += Bsums[i];
		}
		E[t*nCells + cID] = sum / NP;
		Beta_last[cID] = Bsum / NP;
	}

}


int main (int argc, char *argv[]) {


	int dim, ydim;

	double restime;
	struct timeval tdr0, tdr1, tdrMaster;


	// Parse arguments **********************************************

	if (argc < 3) {
		std::cout << "Not enough arguments" << std::endl;
		return 0;
	}

	std::string arg1(argv[1]);
	std::string arg2(argv[2]);

	std::cout << "Arguments:" << std::endl;
	std::cout << "    [1] " << arg1 << std::endl;
	std::cout << "    [2] " << arg2 << std::endl;

	// **************************************************************


	// Read data ****************************************************

	std::cout << "Getting data" << std::endl;

	int * trueCounts 	= getData(arg2, &dim, &ydim);
	int * data 			= getData(arg1, NULL, NULL);

	// **************************************************************


	int T = ydim / dim;
	int nCells = dim*dim;

	size_t datasize = ydim * dim * sizeof(int);


	// populate adjacency matrix **************************************************************************
	// TODO make this a kernel

	std::cout << "Populating adjacency matrix" << std::endl;

	gettimeofday (&tdr0, NULL);

	int adjMat[nCells*8];		// Each row holds the indices of that cell's neighbours
	int inds_check[8] 	= {-(dim+1), -(dim), -(dim-1), -1, 1, (dim-1), (dim), (dim+1)};
	int nNeibVec[nCells];

	for (int cell = 0; cell < nCells; cell++) {

		int neibCtr = 0;

		int i_cell = cell / dim;
		int j_cell = cell % dim;

		for (int j = 0; j < 8; j++) {

			int nei_ind = cell + inds_check[j];
			int i_ref = nei_ind / dim;
			int j_ref = nei_ind % dim;

			if (0 <= nei_ind && nei_ind < nCells && abs(i_cell - i_ref) <= 1 && abs(j_cell - j_ref) <= 1 ) {
				adjMat[cell*8 + neibCtr] = nei_ind;
				neibCtr++;
			}

			if (neibCtr < 8) {
				for (int i = neibCtr; i < 8; i++)
					adjMat[cell*8 + i] = -1;
			}

		}

		nNeibVec[cell] = neibCtr;

	}

	gettimeofday (&tdr1, NULL);
    timeval_subtract (&restime, &tdr1, &tdr0);

    std::cout << "\t" << getHRtime(restime) << std::endl;

	// *****************************************************************************************************


	// CUDA data ****************************************************

	std::cout << "Allocating device storage" << std::endl;

	gettimeofday (&tdr0, NULL);

	int 		* d_data;			// device copy of data
	float 		* d_E;				// device copy of estimates
	Particle 	* particles;		// particles
	Particle 	* particles_old; 	// intermediate particle states
	float 		* Beta_last;		// Estimates of Beta from last iteration
	float 		* w;				// weights
	int         * d_adjMat; 		// device copy of adjacency matrix
	int 		* d_nNeibVec; 		// device copy of neighbour counts matrix

	CUDA_CALL( cudaMalloc( (void**) &d_data 		, datasize )					);
	CUDA_CALL( cudaMalloc( (void**) &d_E 			, nCells*T*sizeof(float)) 		);
	CUDA_CALL( cudaMalloc( (void**) &particles 		, NP*nCells*sizeof(Particle)) 	);
	CUDA_CALL( cudaMalloc( (void**) &particles_old 	, NP*nCells*sizeof(Particle)) 	);
	CUDA_CALL( cudaMalloc( (void**) &Beta_last 		, nCells*sizeof(float)) 		);
	CUDA_CALL( cudaMalloc( (void**) &w 				, nCells*NP*sizeof(float)) 		);
	CUDA_CALL( cudaMalloc( (void**) &d_adjMat 		, nCells*8*sizeof(int)) 		);
	CUDA_CALL( cudaMalloc( (void**) &d_nNeibVec 	, nCells*sizeof(int)) 			);

	gettimeofday (&tdr1, NULL);
    timeval_subtract (&restime, &tdr1, &tdr0);

    std::cout << "\t" << getHRtime(restime) << std::endl;

	size_t avail, total;
	cudaMemGetInfo( &avail, &total );
	size_t used = total - avail;

	std::cout << "\t[" << getHRmemsize(used) << "] used of [" << getHRmemsize(total) << "]" <<std::endl;

	std::cout << "Copying data to device" << std::endl;

	gettimeofday (&tdr0, NULL);

	CUDA_CALL( cudaMemcpy(d_data		, data 		, datasize					, cudaMemcpyHostToDevice)	);
	CUDA_CALL( cudaMemcpy(d_adjMat		, adjMat 	, nCells*8*sizeof(int)		, cudaMemcpyHostToDevice)	);
	CUDA_CALL( cudaMemcpy(d_nNeibVec 	, nNeibVec 	, nCells*sizeof(int)		, cudaMemcpyHostToDevice)	);

	gettimeofday (&tdr1, NULL);
    timeval_subtract (&restime, &tdr1, &tdr0);

    std::cout << "\t" << getHRtime(restime) << std::endl;

	// **************************************************************


	// Initialize particles *****************************************

	std::cout << "Initializing particles" << std::endl;

	gettimeofday (&tdr0, NULL);

	int nThreads 	= 1024;
	int nBlocks 	= (NP*nCells) / nThreads + ( (NP*nCells) % nThreads );

	initializeParticles <<< nBlocks, nThreads >>> (d_data, particles);
	CUDA_CALL( cudaGetLastError() );
	CUDA_CALL( cudaDeviceSynchronize() );

	gettimeofday (&tdr1, NULL);
    timeval_subtract (&restime, &tdr1, &tdr0);

    std::cout << "\t" << getHRtime(restime) << std::endl;


	// **************************************************************


	// Initial estimate by reduction ********************************

	nThreads 	= RB_DIM;
	nBlocks 	= nCells;

	reduce <<< nBlocks, nThreads >>> (d_E, 0, particles, Beta_last, nCells);
	CUDA_CALL( cudaGetLastError() );
	CUDA_CALL( cudaDeviceSynchronize() );

	float * h_E;
	cudaMallocHost( (void**) &h_E, nCells*T*sizeof(float) );
	cudaMemcpy(h_E, d_E, nCells*T*sizeof(float), cudaMemcpyDeviceToHost);

		/*std::cout << "======" << std::endl;
		std::cout << "T = " << 0 << std::endl;
		std::cout << "------" << std::endl;
		for(int i = 0; i < nCells; i++) {
			std::cout << i << " | " << data[i] << "\t" << h_E[i] << std::endl;
		}*/

	// **************************************************************


	// Starting filtering *******************************************

	std::cout << "Filtering over [1," << T << "]"<< std::endl;



	gettimeofday (&tdrMaster, NULL);

	int T_lim = T;

	for (int t = 1; t < T_lim; t++) {

		// Projection ************************************************

		nThreads 	= 1024;
		nBlocks 	= (NP*nCells) / nThreads + ( (NP*nCells) % nThreads );

		if (t == 1)
			gettimeofday (&tdr0, NULL);

		project <<< nBlocks, nThreads >>> (d_data, particles, particles_old, w, d_E, Beta_last, d_adjMat, d_nNeibVec, t-1, nCells);
		CUDA_CALL( cudaGetLastError() );
		CUDA_CALL( cudaDeviceSynchronize() );

		if (t == 1) {
			gettimeofday (&tdr1, NULL);
	    	timeval_subtract (&restime, &tdr1, &tdr0);
	    	std::cout << "Projection       " << getHRtime(restime) << std::endl;
	    }

	    // Cumulative sum ********************************************

		nThreads 	= 32;
		nBlocks 	= nCells / nThreads + ( nCells % nThreads );

		if (t == 1)
			gettimeofday (&tdr0, NULL);

		cumsumWeights <<< nBlocks, nThreads >>> (w, nCells);
		CUDA_CALL( cudaGetLastError() );
		CUDA_CALL( cudaDeviceSynchronize() );

		if (t == 1) {
			gettimeofday (&tdr1, NULL);
	    	timeval_subtract (&restime, &tdr1, &tdr0);
	    	std::cout << "Cumulative sum   " << getHRtime(restime) << std::endl;
	    }

	    // Resampling *************************************************

		nThreads 	= 1024;
		nBlocks 	= (NP*nCells) / nThreads + ( (NP*nCells) % nThreads );

		if (t == 1)
			gettimeofday (&tdr0, NULL);

		resample <<< nBlocks, nThreads >>> (particles, particles_old, w);
		CUDA_CALL( cudaGetLastError() );
		CUDA_CALL( cudaDeviceSynchronize() );

		if (t == 1) {
			gettimeofday (&tdr1, NULL);
	    	timeval_subtract (&restime, &tdr1, &tdr0);
	    	std::cout << "Resampling       " << getHRtime(restime) << std::endl;
	    }

	    // Reduction **************************************************

		nThreads 	= RB_DIM;
		nBlocks 	= nCells;

		if (t == 1)
			gettimeofday (&tdr0, NULL);

		reduce <<< nBlocks, nThreads >>> (d_E, t, particles, Beta_last, nCells);
		CUDA_CALL( cudaGetLastError() );
		CUDA_CALL( cudaDeviceSynchronize() );

		if (t == 1) {
			gettimeofday (&tdr1, NULL);
	    	timeval_subtract (&restime, &tdr1, &tdr0);
	    	std::cout << "Reduction        " << getHRtime(restime) << std::endl;
	    }

	    // Output *****************************************************

		if (t == T_lim/2) {

			cudaMemcpy(h_E, d_E, nCells*T*sizeof(float), cudaMemcpyDeviceToHost);

			std::string filename = "cuSPF.dat";

			std::cout << "Writing results to file '" << filename << "' ..." << std::endl;

			std::ofstream outfile;
			outfile.open(filename.c_str());

			for (int i = 0; i < nCells; i++) {
				outfile << trueCounts[t*nCells + i];
				if (i % dim == 0)
					outfile << std::endl;
				else
					outfile << " ";
			}
			
			for (int i = 0; i < nCells; i++) {
				outfile << data[t*nCells + i];
				if (i % dim == 0)
					outfile << std::endl;
				else
					outfile << " ";
			}

			for (int i = 0; i < nCells; i++) {
				outfile << h_E[t*nCells + i];
				if (i % dim == 0)
					outfile << std::endl;
				else
					outfile << " ";
			}

			outfile.close();
			
		}

	}

	gettimeofday (&tdr1, NULL);
	timeval_subtract (&restime, &tdr1, &tdrMaster);
	std::cout << "Total PF time (excluding setup) " << getHRtime(restime) << std::endl;

	/*int T_last = T_lim - 1;

	std::cout << "======" << std::endl;
	std::cout << "T = " << T_last << std::endl;
	std::cout << "------" << std::endl;
	cudaMemcpy(h_E, d_E, nCells*T*sizeof(float), cudaMemcpyDeviceToHost);
	for(int i = 0; i < nCells; i++) {
		std::cout << i << " | " << data[nCells*T_last + i] << "\t" << h_E[nCells*T_last + i] << std::endl;
	}*/

	/*int printID = 45;

	cudaMemcpy(h_E, d_E, nCells*T*sizeof(float), cudaMemcpyDeviceToHost);
	for (int i = 0; i < T; i++)
		std::cout << data[i*nCells + printID] << " " <<h_E[i*nCells + printID] << std::endl;
	*/
	/*

	int BLOCK_DIM = 1024; // should be the maximum number of threads per multiprocessor
	int NUM_BLOCKS = NP / BLOCK_DIM + (NP % BLOCK_DIM);

	initializeParticles <<< NUM_BLOCKS, BLOCK_DIM >>> (i_infec);
	CUDA_CALL( cudaGetLastError() );
	CUDA_CALL( cudaDeviceSynchronize() );

	for (int t = 1; t < T; t++) {

		project <<< NUM_BLOCKS, BLOCK_DIM >>> (t);
		CUDA_CALL( cudaGetLastError() );
		CUDA_CALL( cudaDeviceSynchronize() );

		cumsumWeights <<< NUM_BLOCKS, BLOCK_DIM >>> ();
		CUDA_CALL( cudaGetLastError() );
		CUDA_CALL( cudaDeviceSynchronize() );

		resample <<< NUM_BLOCKS, BLOCK_DIM >>> (t);
		CUDA_CALL( cudaGetLastError() );
		CUDA_CALL( cudaDeviceSynchronize() );

	}

	int R_BLOCKS = T / BLOCK_DIM + (T % BLOCK_DIM);

	size_t avail, total;
	cudaMemGetInfo( &avail, &total );
	size_t used = total - avail;
	

	reduce <<< R_BLOCKS, BLOCK_DIM >>> (T);
	CUDA_CALL( cudaGetLastError() );
	CUDA_CALL( cudaDeviceSynchronize() );

	gettimeofday (&tdr1, NULL);
    timeval_subtract (&restime, &tdr1, &tdr0);

    cout << "---------------------------" << endl;
    cout << "CUDA STATS" << endl;
    cout << endl;
    cout << "Runtime          " << getHRtime(restime) << endl;
    cout << "Device mem used  " << getHRmemsize(used) << endl;
    cout << "---------------------------" << endl;

	float h_y_save[T];
	CUDA_CALL( cudaMemcpyFromSymbol(h_y_save, y_save, T*sizeof(float), 0, cudaMemcpyDeviceToHost) );
	CUDA_CALL( cudaDeviceSynchronize() );

	string filename = "pf.dat";

	cout << "Writing results to file '" << filename << "' ..." << endl;

	ofstream outfile;
	outfile.open(filename.c_str());
	
	for (int t = 0; t < T; t++)
		outfile << t << " " << y_true[t] << " " << y_noise[t] << " " << h_y_save[t] << endl;  

	outfile.close();

	if (plotFlag) {

		cout << "Plotting using gnuplot ..." << endl;
		cout << "Press ENTER to close plot and exit" << endl;

		string syscall("gnuplot -e \"filename='");
		syscall += filename;
		syscall += "'\" pf.plg";

		system( syscall.c_str() );

	}

	exit (EXIT_SUCCESS);

	*/

}


/*	Use the Explicit Euler integration scheme to integrate SIR model forward in time
	float h 	- time step size
	float t0 	- start time
	float tn 	- stop time
	float * y 	- current system state; a three-component vector representing [S I R], susceptible-infected-recovered
	*/
__device__ void exp_euler_SIR(float h, float t0, float tn,
					Particle * particles, int pID,
					float * I_last, float * Beta_last, int * adjMat, int * nNeibVec, int cID) {
	
	int num_steps = floor( (tn-t0) / h );

	float S 			= particles[pID].S;
	float I 			= particles[pID].I;
	float R 			= particles[pID].R;
	float Beta 			= particles[pID].Beta;
	curandState state 	= particles[pID].randState;

	int nNei = nNeibVec[cID];

	float nInfec_sum = 0;
	for (int n = 0; n < nNei; n++) {
		int nID = adjMat[cID*8 + n];
		nInfec_sum += Beta_last[nID] * I_last[nID];
	}

	// let Beta walk by about 10% of its value at a time, don't let it go negative
	Beta += (Beta/10.0)*curand_normal(&state);
	if (Beta < 0)
		Beta = 0;

	for(int i = 0; i < num_steps; i++) {
		// get derivatives
		float dS = - (1-nNei*alp)*Beta*S*I - alp*S*nInfec_sum;
		float dI = - dS - r*I;
		float dR = r*I;
		// step forward by h
		S += h*dS;
		I += h*dI;
		R += h*dR;
	}

	particles[pID].S = S;
	particles[pID].I = I;
	particles[pID].R = R;
	particles[pID].Beta = Beta;

}


/*	Convert memory size in bytes to human-readable format
	*/
std::string getHRmemsize (size_t memsize) {

	std::stringstream ss;
	std::string valstring;

	int kb = 1024;
	int mb = kb*1024;
	int gb = mb*1024;
	
	if (memsize <= kb)
		ss << memsize << " B";
	else if (memsize > kb && memsize <= mb)
		ss << (float) memsize/ kb << " KB";
	else if (memsize > mb && memsize <= gb)
		ss << (float) memsize/ mb << " MB";
	else
		ss << (float) memsize/ gb << " GB";

	valstring = ss.str();
	
	return valstring;

}


/*	Convert time in seconds to human readable format
	*/
std::string getHRtime (double runtime) {

	std::stringstream ss;
	std::string valstring;

	int mt = 60;
	int ht = mt*60;
	int dt = ht*24;
	
	if (runtime <= mt)
		ss << runtime << " s";
	else if (runtime > mt && runtime <= ht)
		ss << runtime/mt << " m";
	else if (runtime > ht && runtime <= dt)
		ss << runtime/dt << " h";
	else
		ss << runtime/ht << " d";

	valstring = ss.str();
	
	return valstring;

}